{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b222152",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Gradient Descent is an iterative optimization method to find a minimum of a function. The idea behind this algorithm is to choose a start point x0, calculate the gradient and then taking little steps in the direction of the negative gradient in each iteration.\n",
    "![Function 6th order](pics/graph6s.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760bc6d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa7fd3",
   "metadata": {},
   "source": [
    "## Define Function $f(x)=x^2-4x+1$\n",
    "Replace the none with your code\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e636bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x:float):\n",
    "    return x**2-4*x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432368d",
   "metadata": {},
   "source": [
    "## Define Derivativ  $f'(x)$\n",
    "Replace the none with your code\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709205d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_func1(x:float):\n",
    "    return 2*x - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d595748",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "Implement the gradient descent algorithm.\n",
    "![gradien descent pseudo code](pics/gdpseudo.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b36d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start: float, gradient: Callable[[float], float],\n",
    "                     learn_rate: float, max_iter: int, tol: float = 0.01):\n",
    "    x = start\n",
    "    steps = [start]  \n",
    "    # Place your code here\n",
    "    for _ in range(max_iter):\n",
    "        diff = learn_rate * gradient(x)\n",
    "        if np.abs(diff) < tol:\n",
    "            break\n",
    "        x = x - diff\n",
    "        steps.append(x)  \n",
    "\n",
    "    return steps, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f46b0",
   "metadata": {},
   "source": [
    "## Results\n",
    "Modify startvalue $x_0$, learning rate $\\alpha$, maximum number of iterations and tolerance. What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a332da27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 2.0490178333326154 after iterations: 250\n"
     ]
    }
   ],
   "source": [
    "x0 = 9.5\n",
    "alpha = 0.01\n",
    "n = 1000\n",
    "t = 0.001\n",
    "history, result = gradient_descent(x0, gradient_func1, alpha, n, t)\n",
    "print('Minimum: ' + str(result) + ' after iterations: ' + str(len(history)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
